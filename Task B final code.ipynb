{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb5eec6",
   "metadata": {},
   "source": [
    "Final CODE FOR TASK B\n",
    "\n",
    "Import necessary libraries & functions for Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da985c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import shutil\n",
    "\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718dde1",
   "metadata": {},
   "source": [
    "Tumour Image Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ff0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_formatting_needed():\n",
    "    #use this function if pickle file for sorted training dataset TaskB already exists\n",
    "    #Initialising Parameters for future steps\n",
    "    base_name = 'TaskB_training_dataset'\n",
    "    width = 256\n",
    "    include = {'glioma_tumor', 'meningioma_tumour', 'no_tumour', 'pituitary_tumor'}\n",
    "    \n",
    "def no_formatting_needed_testset():\n",
    "    #use this function if pickle file for sorted test dataset TaskB already exists\n",
    "    #Initialising Parameters for future steps\n",
    "    base_name = 'TaskB_test_dataset'\n",
    "    width = 256\n",
    "    include = {'glioma_tumor', 'meningioma_tumour', 'no_tumour', 'pituitary_tumor'}\n",
    "    print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42d6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_needed():\n",
    "    #use this function if pickle file for sorted trainig dataset TaskB does not exist (i.e first time of running the code)\n",
    "    \n",
    "    #Function to preprocess the tumour images (grayscale, resize, convert to array and storing in pickle file)\n",
    "    def format_img(src, pklname, include, width=150, height=None):\n",
    "        \"\"\"\n",
    "        function to load images from source, convert to grayscale, resize them and \n",
    "        write them as arrays to a dictionary with their respective labels. \n",
    "        The dictionary is written to a pickle file.\n",
    "     \n",
    "        Parameter\n",
    "        ---------\n",
    "        src: str\n",
    "            path to data\n",
    "        pklname: str\n",
    "            path to output file\n",
    "        width: int\n",
    "            target width of the image in pixels\n",
    "        include: set[str]\n",
    "            set containing str\n",
    "        \"\"\"\n",
    "     \n",
    "        height = height if height is not None else width\n",
    "     \n",
    "        data = dict()\n",
    "        data['label'] = []\n",
    "        data['filename'] = []\n",
    "        data['data'] = []   \n",
    "     \n",
    "        pklname = f\"{pklname}_{width}x{height}px.pkl\"\n",
    " \n",
    "        # read all images in PATH, convert to greyscale, resize and write to destination\n",
    "        for subdir in os.listdir(src):\n",
    "            if subdir in include:\n",
    "                print(subdir)\n",
    "                current_path = os.path.join(src, subdir)\n",
    " \n",
    "                for file in os.listdir(current_path):\n",
    "                    if file[-3:] in {'jpg', 'png'}:\n",
    "                        im = imread(os.path.join(current_path, file), as_gray=True) \n",
    "                        im = resize(im, (width, height))\n",
    "                        data['label'].append(subdir[:-4])\n",
    "                        data['filename'].append(file)\n",
    "                        data['data'].append(im)\n",
    " \n",
    "            joblib.dump(data, pklname)\n",
    "    \n",
    "    #Sorting the images to four folders\n",
    "    #Import label.csv file\n",
    "    data = pd.read_csv('./dataset./label.csv')\n",
    "    #print(data) #debug line\n",
    "\n",
    "    #Give the image labels values based on whether there is tumour and what type of tumour\n",
    "    data.label[data.label == 'no_tumor'] = 0\n",
    "    data.label[data.label == 'meningioma_tumor'] = 1\n",
    "    data.label[data.label == 'glioma_tumor'] = 2\n",
    "    data.label[data.label == 'pituitary_tumor'] = 3\n",
    "    #print(data.dtypes) #debug line\n",
    "\n",
    "    #Convert the label data type to integer\n",
    "    data['label'] = data['label'].astype(str).astype(int)\n",
    "    #print(data.dtypes) #debug line\n",
    "    #print(data) #debug line\n",
    "\n",
    "    #Create four arrays which contain the respective image names  \n",
    "    no_tumour = []\n",
    "    meningioma_tumour = []\n",
    "    glioma_tumor = []\n",
    "    pituitary_tumor = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        if data['label'][i] == 0:\n",
    "            no_tumour.append(data['file_name'][i])\n",
    "        elif data['label'][i] == 1:\n",
    "            meningioma_tumour.append(data['file_name'][i])\n",
    "        elif data['label'][i] == 2:\n",
    "            glioma_tumor.append(data['file_name'][i])\n",
    "        else:\n",
    "            pituitary_tumor.append(data['file_name'][i])\n",
    "\n",
    "    #print(len(no_tumour)) #debug line\n",
    "    #print(len(meningioma_tumour)) #debug line\n",
    "    #print(len(glioma_tumor)) #debug line\n",
    "    #print(len(pituitary_tumor)) #debug line\n",
    "\n",
    "    #Create a sorted subfolder to store the sorted images\n",
    "    os.mkdir(os.path.join(\"./dataset./image\", \"TaskB_sortedtrainingdataset\"))\n",
    "\n",
    "    #Create four subfolders in sorted folder\n",
    "    os.mkdir(os.path.join(\"./dataset./image/TaskB_sortedtrainingdataset\", \"no_tumour\"))\n",
    "    os.mkdir(os.path.join(\"./dataset./image/TaskB_sortedtrainingdataset\", \"meningioma_tumour\"))\n",
    "    os.mkdir(os.path.join(\"./dataset./image/TaskB_sortedtrainingdataset\", \"glioma_tumor\"))\n",
    "    os.mkdir(os.path.join(\"./dataset./image/TaskB_sortedtrainingdataset\", \"pituitary_tumor\"))\n",
    "\n",
    "    #Copy the images into the respective subfolders\n",
    "    src_dir = \"./dataset./image\"\n",
    "    dst_dir1 = os.path.join(\"./dataset./image/TaskB_sortedtrainingdataset\", \"no_tumour\")\n",
    "    dst_dir2 = os.path.join(\"./dataset./image/TaskB_sortedtrainingdataset\", \"meningioma_tumour\")\n",
    "    dst_dir3 = os.path.join(\"./dataset./image/TaskB_sortedtrainingdataset\", \"glioma_tumor\")\n",
    "    dst_dir4 = os.path.join(\"./dataset./image/TaskB_sortedtrainingdataset\", \"pituitary_tumor\")\n",
    "    data_path = \"./dataset./image/TaskB_sortedtrainingdataset\"\n",
    "\n",
    "    for imageName in no_tumour:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir1)\n",
    "\n",
    "    for imageName in meningioma_tumour:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir2)\n",
    "\n",
    "    for imageName in glioma_tumor:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir3)\n",
    "\n",
    "    for imageName in pituitary_tumor:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir4)\n",
    "\n",
    "    #os.listdir(data_path)  #debug line\n",
    "\n",
    "    #Initialising Parameters for resize function\n",
    "    base_name = 'TaskB_training_dataset'\n",
    "    width = 256\n",
    "    include = {'glioma_tumor', 'meningioma_tumour', 'no_tumour', 'pituitary_tumor'}\n",
    " \n",
    "    #Create pkl file with resized images resized and converted to arrays\n",
    "    format_img(src=data_path, pklname=base_name, width=width, include=include)\n",
    "    print(\"Step Complete: Training dataset formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f98fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_needed_testset():\n",
    "    #use this function if pickle file for sorted trainig dataset TaskB does not exist (i.e first time of running the code)\n",
    "    \n",
    "    #Function to preprocess the tumour images (grayscale, resize, convert to array and storing in pickle file)\n",
    "    def format_img(src, pklname, include, width=150, height=None):\n",
    "        \"\"\"\n",
    "        function to load images from source, convert to grayscale, resize them and \n",
    "        write them as arrays to a dictionary with their respective labels. \n",
    "        The dictionary is written to a pickle file.\n",
    "     \n",
    "        Parameter\n",
    "        ---------\n",
    "        src: str\n",
    "            path to data\n",
    "        pklname: str\n",
    "            path to output file\n",
    "        width: int\n",
    "            target width of the image in pixels\n",
    "        include: set[str]\n",
    "            set containing str\n",
    "        \"\"\"\n",
    "     \n",
    "        height = height if height is not None else width\n",
    "     \n",
    "        data = dict()\n",
    "        data['label'] = []\n",
    "        data['filename'] = []\n",
    "        data['data'] = []   \n",
    "     \n",
    "        pklname = f\"{pklname}_{width}x{height}px.pkl\"\n",
    " \n",
    "        # read all images in PATH, convert to greyscale, resize and write to destination\n",
    "        for subdir in os.listdir(src):\n",
    "            if subdir in include:\n",
    "                print(subdir)\n",
    "                current_path = os.path.join(src, subdir)\n",
    " \n",
    "                for file in os.listdir(current_path):\n",
    "                    if file[-3:] in {'jpg', 'png'}:\n",
    "                        im = imread(os.path.join(current_path, file), as_gray=True) \n",
    "                        im = resize(im, (width, height))\n",
    "                        data['label'].append(subdir[:-4])\n",
    "                        data['filename'].append(file)\n",
    "                        data['data'].append(im)\n",
    " \n",
    "            joblib.dump(data, pklname)\n",
    "    \n",
    "    #Sorting the images to four folders\n",
    "    #Import label.csv file\n",
    "    data = pd.read_csv('./test./label.csv')\n",
    "    #print(data) #debug line\n",
    "\n",
    "    #Give the image labels values based on whether there is tumour and what type of tumour\n",
    "    data.label[data.label == 'no_tumor'] = 0\n",
    "    data.label[data.label == 'meningioma_tumor'] = 1\n",
    "    data.label[data.label == 'glioma_tumor'] = 2\n",
    "    data.label[data.label == 'pituitary_tumor'] = 3\n",
    "    #print(data.dtypes) #debug line\n",
    "\n",
    "    #Convert the label data type to integer\n",
    "    data['label'] = data['label'].astype(str).astype(int)\n",
    "    #print(data.dtypes) #debug line\n",
    "    #print(data) #debug line\n",
    "\n",
    "    #Create four arrays which contain the respective image names  \n",
    "    no_tumour = []\n",
    "    meningioma_tumour = []\n",
    "    glioma_tumor = []\n",
    "    pituitary_tumor = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        if data['label'][i] == 0:\n",
    "            no_tumour.append(data['file_name'][i])\n",
    "        elif data['label'][i] == 1:\n",
    "            meningioma_tumour.append(data['file_name'][i])\n",
    "        elif data['label'][i] == 2:\n",
    "            glioma_tumor.append(data['file_name'][i])\n",
    "        else:\n",
    "            pituitary_tumor.append(data['file_name'][i])\n",
    "\n",
    "    #print(len(no_tumour)) #debug line\n",
    "    #print(len(meningioma_tumour)) #debug line\n",
    "    #print(len(glioma_tumor)) #debug line\n",
    "    #print(len(pituitary_tumor)) #debug line\n",
    "\n",
    "    #Create a sorted subfolder to store the sorted images\n",
    "    os.mkdir(os.path.join(\"./test./image\", \"TaskB_sortedtestdataset\"))\n",
    "\n",
    "    #Create four subfolders in sorted folder\n",
    "    os.mkdir(os.path.join(\"./test./image/TaskB_sortedtestdataset\", \"no_tumour\"))\n",
    "    os.mkdir(os.path.join(\"./test./image/TaskB_sortedtestdataset\", \"meningioma_tumour\"))\n",
    "    os.mkdir(os.path.join(\"./test./image/TaskB_sortedtestdataset\", \"glioma_tumor\"))\n",
    "    os.mkdir(os.path.join(\"./test./image/TaskB_sortedtestdataset\", \"pituitary_tumor\"))\n",
    "\n",
    "    #Copy the images into the respective subfolders\n",
    "    src_dir = \"./test./image\"\n",
    "    dst_dir1 = os.path.join(\"./test./image/TaskB_sortedtestdataset\", \"no_tumour\")\n",
    "    dst_dir2 = os.path.join(\"./test./image/TaskB_sortedtestdataset\", \"meningioma_tumour\")\n",
    "    dst_dir3 = os.path.join(\"./test./image/TaskB_sortedtestdataset\", \"glioma_tumor\")\n",
    "    dst_dir4 = os.path.join(\"./test./image/TaskB_sortedtestdataset\", \"pituitary_tumor\")\n",
    "    data_path = \"./test./image/TaskB_sortedtestdataset\"\n",
    "\n",
    "    for imageName in no_tumour:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir1)\n",
    "\n",
    "    for imageName in meningioma_tumour:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir2)\n",
    "\n",
    "    for imageName in glioma_tumor:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir3)\n",
    "\n",
    "    for imageName in pituitary_tumor:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir4)\n",
    "\n",
    "    #os.listdir(data_path)  #debug line\n",
    "\n",
    "    #Initialising Parameters for resize function\n",
    "    base_name = 'TaskB_test_dataset'\n",
    "    width = 256\n",
    "    include = {'glioma_tumor', 'meningioma_tumour', 'no_tumour', 'pituitary_tumor'}\n",
    " \n",
    "    #Create pkl file with resized images resized and converted to arrays\n",
    "    format_img(src=data_path, pklname=base_name, width=width, include=include)\n",
    "    print(\"Step Complete: Test dataset formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195c9eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does a pickle file containing the sorted training data exist?(Please enter N in captial if running code for first time and Y in capital for subsequent runs) N\n",
      "glioma_tumor\n",
      "meningioma_tumour\n",
      "no_tumour\n",
      "pituitary_tumor\n",
      "Step Complete: Training dataset formatted\n",
      "glioma_tumor\n",
      "meningioma_tumour\n",
      "no_tumour\n",
      "pituitary_tumor\n",
      "Step Complete: Test dataset formatted\n"
     ]
    }
   ],
   "source": [
    "image_formatting=input(\"Does a pickle file containing the sorted training data exist?(Please enter N in captial if running code for first time and Y in capital for subsequent runs) \")\n",
    "\n",
    "if image_formatting == \"Y\":\n",
    "    no_formatting_needed()\n",
    "    no_formatting_needed_testset()\n",
    "elif image_formatting == \"N\":\n",
    "    formatting_needed()\n",
    "    formatting_needed_testset()\n",
    "else:\n",
    "    print(\"Incorrect input! Please run cell again and input either Y or N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3eb1a8",
   "metadata": {},
   "source": [
    "Load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a61d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset\n",
      "number of samples:  3000\n",
      "image resolution:  (256, 256)\n",
      "labels: ['glioma_t' 'meningioma_tu' 'no_tu' 'pituitary_t']\n",
      "\n",
      "Test dataset\n",
      "number of samples:  200\n",
      "image resolution:  (256, 256)\n",
      "labels: ['glioma_t' 'meningioma_tu' 'no_tu' 'pituitary_t']\n",
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "#Initialising Parameters for resize function\n",
    "base_name = 'TaskB_training_dataset'\n",
    "width = 256\n",
    "include = {'glioma_tumor', 'meningioma_tumour', 'no_tumour', 'pituitary_tumor'}\n",
    "\n",
    "#Load pickle data file\n",
    "data = joblib.load(f'{base_name}_{width}x{width}px.pkl')\n",
    "\n",
    "#some information about the data\n",
    "print('Training dataset')\n",
    "print('number of samples: ', len(data['data']))\n",
    "print('image resolution: ', data['data'][0].shape)\n",
    "print('labels:', np.unique(data['label']))\n",
    "\n",
    "#Initialising some parameters\n",
    "base_name = 'TaskB_test_dataset'\n",
    "width = 256\n",
    "include = {'glioma_tumor', 'meningioma_tumour', 'no_tumour', 'pituitary_tumor'}\n",
    "\n",
    "#Load pickle file\n",
    "data_test = joblib.load(f'{base_name}_{width}x{width}px.pkl')\n",
    "\n",
    "#some information about the data\n",
    "print('')\n",
    "print('Test dataset')\n",
    "print('number of samples: ', len(data_test['data']))\n",
    "print('image resolution: ', data_test['data'][0].shape)\n",
    "print('labels:', np.unique(data_test['label']))\n",
    "\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d9e82",
   "metadata": {},
   "source": [
    "Split the data to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5212f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "#Extract the X and y values from data\n",
    "X = np.array(data['data'])\n",
    "y = np.array(data['label'])\n",
    "\n",
    "#Split data to train and test (75% train, 25% split)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.25, \n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "#Extract the X and y values from the test data\n",
    "X_test = np.array(data_test['data'])\n",
    "y_test = np.array(data_test['label'])\n",
    "\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172a280",
   "metadata": {},
   "source": [
    "Define the HOG transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d348eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "class HogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Expects an array of 2d arrays (greyscale or 1 channel images)\n",
    "    Calculates hog features for each img\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, y=None, orientations=9,\n",
    "                 pixels_per_cell=(8, 8),\n",
    "                 cells_per_block=(3, 3), block_norm='L2-Hys'):\n",
    "        self.y = y\n",
    "        self.orientations = orientations\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "        self.block_norm = block_norm\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    " \n",
    "        def local_hog(X):\n",
    "            return hog(X,\n",
    "                       orientations=self.orientations,\n",
    "                       pixels_per_cell=self.pixels_per_cell,\n",
    "                       cells_per_block=self.cells_per_block,\n",
    "                       block_norm=self.block_norm)\n",
    " \n",
    "        try: # parallel\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "        except:\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "        \n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d93d3",
   "metadata": {},
   "source": [
    "Prepare training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f256db9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "# create an instance of each transformer\n",
    "hogify = HogTransformer(\n",
    "    #change parameters below for optimisation\n",
    "    pixels_per_cell=(12, 12), \n",
    "    cells_per_block=(3, 3), \n",
    "    orientations=9, \n",
    "    block_norm='L2-Hys'\n",
    ")\n",
    "scalify = StandardScaler()\n",
    " \n",
    "# call fit_transform on each transform converting X_train step by step\n",
    "X_train_hog = hogify.fit_transform(X_train)\n",
    "X_train_prepared = scalify.fit_transform(X_train_hog)\n",
    "#print(X_train.shape) #debug line\n",
    "#print(X_train_prepared.shape) #debug line\n",
    "\n",
    "#transform validation data\n",
    "X_validate_hog = hogify.transform(X_validate)\n",
    "X_validate_prepared = scalify.transform(X_validate_hog)\n",
    "#print(X_validate_prepared.shape)  #debug line\n",
    "\n",
    "#transform test data\n",
    "X_test_hog = hogify.transform(X_test)\n",
    "X_test_prepared = scalify.transform(X_test_hog)\n",
    "#print(X_test_prepared.shape) #debug line\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841af7a3",
   "metadata": {},
   "source": [
    "SVM with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4810422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 9235.61, NNZs: 29241, Bias: -2169.626376, T: 2250, Avg. loss: 10458.018704\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6453.91, NNZs: 29241, Bias: -2517.409860, T: 4500, Avg. loss: 1607.843452\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4975.94, NNZs: 29241, Bias: -2609.407806, T: 6750, Avg. loss: 360.654235\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4068.70, NNZs: 29241, Bias: -2677.482173, T: 9000, Avg. loss: 199.681065\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3429.85, NNZs: 29241, Bias: -2700.633037, T: 11250, Avg. loss: 69.560479\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2937.42, NNZs: 29241, Bias: -2708.796401, T: 13500, Avg. loss: 35.903403\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2576.82, NNZs: 29241, Bias: -2707.579520, T: 15750, Avg. loss: 21.459965\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2290.86, NNZs: 29241, Bias: -2709.184235, T: 18000, Avg. loss: 7.378538\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2060.53, NNZs: 29241, Bias: -2707.251281, T: 20250, Avg. loss: 6.915051\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1877.19, NNZs: 29241, Bias: -2706.359183, T: 22500, Avg. loss: 3.620042\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1724.37, NNZs: 29241, Bias: -2705.565459, T: 24750, Avg. loss: 5.808919\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1592.68, NNZs: 29241, Bias: -2704.090241, T: 27000, Avg. loss: 0.590096\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1481.07, NNZs: 29241, Bias: -2703.423242, T: 29250, Avg. loss: 2.024569\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1387.35, NNZs: 29241, Bias: -2702.795242, T: 31500, Avg. loss: 1.760239\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1309.09, NNZs: 29241, Bias: -2700.704315, T: 33750, Avg. loss: 2.590512\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1231.94, NNZs: 29241, Bias: -2700.702149, T: 36000, Avg. loss: 0.195236\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1164.01, NNZs: 29241, Bias: -2700.181377, T: 38250, Avg. loss: 0.172157\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1103.37, NNZs: 29241, Bias: -2699.692628, T: 40500, Avg. loss: 0.146473\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1049.24, NNZs: 29241, Bias: -2699.456817, T: 42750, Avg. loss: 0.616845\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 997.92, NNZs: 29241, Bias: -2699.456817, T: 45000, Avg. loss: 0.000000\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 959.89, NNZs: 29241, Bias: -2697.982487, T: 47250, Avg. loss: 1.244580\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 920.40, NNZs: 29241, Bias: -2697.784434, T: 49500, Avg. loss: 1.056549\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 886.87, NNZs: 29241, Bias: -2696.819656, T: 51750, Avg. loss: 0.864991\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 853.62, NNZs: 29241, Bias: -2696.445273, T: 54000, Avg. loss: 0.349848\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 827.25, NNZs: 29241, Bias: -2695.368174, T: 56250, Avg. loss: 0.763408\n",
      "Total training time: 2.03 seconds.\n",
      "Convergence after 25 epochs took 2.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 9454.62, NNZs: 29241, Bias: -2585.754640, T: 2250, Avg. loss: 13811.199241\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6811.38, NNZs: 29241, Bias: -3246.783625, T: 4500, Avg. loss: 2847.939483\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5359.63, NNZs: 29241, Bias: -3499.254912, T: 6750, Avg. loss: 808.601139\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4429.90, NNZs: 29241, Bias: -3640.244493, T: 9000, Avg. loss: 389.948739\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3766.32, NNZs: 29241, Bias: -3693.296159, T: 11250, Avg. loss: 164.763105\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3231.00, NNZs: 29241, Bias: -3707.166367, T: 13500, Avg. loss: 52.189171\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2829.17, NNZs: 29241, Bias: -3716.163561, T: 15750, Avg. loss: 26.881074\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2515.54, NNZs: 29241, Bias: -3719.923494, T: 18000, Avg. loss: 12.020043\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2263.59, NNZs: 29241, Bias: -3719.996311, T: 20250, Avg. loss: 13.084646\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2055.31, NNZs: 29241, Bias: -3719.563681, T: 22500, Avg. loss: 2.068070\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1903.14, NNZs: 29241, Bias: -3713.488369, T: 24750, Avg. loss: 5.319945\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1756.95, NNZs: 29241, Bias: -3712.380073, T: 27000, Avg. loss: 1.809705\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1637.82, NNZs: 29241, Bias: -3709.614932, T: 29250, Avg. loss: 3.117840\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1529.49, NNZs: 29241, Bias: -3708.966328, T: 31500, Avg. loss: 1.783097\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1438.18, NNZs: 29241, Bias: -3707.481275, T: 33750, Avg. loss: 0.896263\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1357.50, NNZs: 29241, Bias: -3706.116122, T: 36000, Avg. loss: 1.661971\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1295.44, NNZs: 29241, Bias: -3703.238925, T: 38250, Avg. loss: 3.413896\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1239.50, NNZs: 29241, Bias: -3701.028561, T: 40500, Avg. loss: 3.115527\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1188.31, NNZs: 29241, Bias: -3698.940972, T: 42750, Avg. loss: 1.729782\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1138.95, NNZs: 29241, Bias: -3697.401755, T: 45000, Avg. loss: 1.078921\n",
      "Total training time: 1.75 seconds.\n",
      "Convergence after 20 epochs took 1.75 seconds\n",
      "-- Epoch 1\n",
      "Norm: 9004.23, NNZs: 29241, Bias: -3869.098260, T: 2250, Avg. loss: 15597.437568\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6452.65, NNZs: 29241, Bias: -4755.672454, T: 4500, Avg. loss: 3241.167046\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5092.58, NNZs: 29241, Bias: -5186.306768, T: 6750, Avg. loss: 1275.725972\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4140.26, NNZs: 29241, Bias: -5356.763259, T: 9000, Avg. loss: 402.834836\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3483.06, NNZs: 29241, Bias: -5427.699772, T: 11250, Avg. loss: 117.465724\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2966.85, NNZs: 29241, Bias: -5445.886169, T: 13500, Avg. loss: 24.798654\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2572.62, NNZs: 29241, Bias: -5451.636830, T: 15750, Avg. loss: 5.852530\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2267.95, NNZs: 29241, Bias: -5451.636830, T: 18000, Avg. loss: 0.000000\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2027.81, NNZs: 29241, Bias: -5451.636830, T: 20250, Avg. loss: 0.000000\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1836.20, NNZs: 29241, Bias: -5451.200701, T: 22500, Avg. loss: 0.118687\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1681.50, NNZs: 29241, Bias: -5450.795920, T: 24750, Avg. loss: 0.651633\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1546.38, NNZs: 29241, Bias: -5450.795920, T: 27000, Avg. loss: 0.000000\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1438.54, NNZs: 29241, Bias: -5449.782885, T: 29250, Avg. loss: 0.393784\n",
      "Total training time: 1.19 seconds.\n",
      "Convergence after 13 epochs took 1.19 seconds\n",
      "-- Epoch 1\n",
      "Norm: 7893.85, NNZs: 29241, Bias: -1950.672045, T: 2250, Avg. loss: 6920.284696\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5697.17, NNZs: 29241, Bias: -2392.652552, T: 4500, Avg. loss: 1289.846075\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4444.33, NNZs: 29241, Bias: -2550.834463, T: 6750, Avg. loss: 368.606228\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3554.69, NNZs: 29241, Bias: -2591.456540, T: 9000, Avg. loss: 80.597048\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2959.08, NNZs: 29241, Bias: -2612.508381, T: 11250, Avg. loss: 28.856438\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2518.91, NNZs: 29241, Bias: -2618.344404, T: 13500, Avg. loss: 5.550245\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2189.58, NNZs: 29241, Bias: -2620.877526, T: 15750, Avg. loss: 3.018537\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1935.18, NNZs: 29241, Bias: -2623.121067, T: 18000, Avg. loss: 0.596947\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1738.89, NNZs: 29241, Bias: -2625.067114, T: 20250, Avg. loss: 1.101433\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1575.58, NNZs: 29241, Bias: -2626.032086, T: 22500, Avg. loss: 1.622877\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1441.49, NNZs: 29241, Bias: -2626.428888, T: 24750, Avg. loss: 0.385372\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1327.54, NNZs: 29241, Bias: -2626.071451, T: 27000, Avg. loss: 0.166590\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1229.02, NNZs: 29241, Bias: -2626.421101, T: 29250, Avg. loss: 0.080034\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1143.93, NNZs: 29241, Bias: -2626.421101, T: 31500, Avg. loss: 0.000000\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1069.86, NNZs: 29241, Bias: -2626.421101, T: 33750, Avg. loss: 0.000000\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1004.80, NNZs: 29241, Bias: -2626.421101, T: 36000, Avg. loss: 0.000000\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 951.20, NNZs: 29241, Bias: -2626.158379, T: 38250, Avg. loss: 0.555047\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 899.63, NNZs: 29241, Bias: -2626.158379, T: 40500, Avg. loss: 0.000000\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 853.36, NNZs: 29241, Bias: -2626.158379, T: 42750, Avg. loss: 0.000000\n",
      "Total training time: 1.64 seconds.\n",
      "Convergence after 19 epochs took 1.64 seconds\n",
      "\n",
      "Running the classifier with validation data\n",
      "Prediction accuracy (%):  89.6\n",
      "[[ 96   3   6   5]\n",
      " [  7 205  17   3]\n",
      " [  7  17 170  10]\n",
      " [  0   2   1 201]]\n",
      "\n",
      "Running the classifier with test data\n",
      "Prediction accuracy (%):  88.5\n",
      "[[34  0  2  1]\n",
      " [ 3 37  3  0]\n",
      " [ 2  5 55  6]\n",
      " [ 0  1  0 51]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    6.5s finished\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-3, verbose=1, learning_rate='optimal', alpha=0.0001)\n",
    "\n",
    "#Fit the data\n",
    "sgd_clf.fit(X_train_prepared, y_train)\n",
    "\n",
    "#Validate the data\n",
    "y_pred = sgd_clf.predict(X_validate_prepared)\n",
    "#print(np.array(y_pred == y_test)[:25])\n",
    "print('')\n",
    "print('Running the classifier with validation data')\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred == y_validate)/len(y_validate))\n",
    "cmx = confusion_matrix(y_validate, y_pred, labels=[\"no_tu\",\"glioma_t\",\"meningioma_tu\",\"pituitary_t\"])\n",
    "print(cmx)\n",
    "\n",
    "#Run the classifier with test data\n",
    "y_pred_test = sgd_clf.predict(X_test_prepared)\n",
    "#print(np.array(y_pred_test == y_test)[:25])\n",
    "print('')\n",
    "print('Running the classifier with test data')\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_test == y_test)/len(y_test))\n",
    "cmx = confusion_matrix(y_test, y_pred_test, labels=[\"no_tu\",\"glioma_t\",\"meningioma_tu\",\"pituitary_t\"])\n",
    "print(cmx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca921769",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db73b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 21\n",
      "Running the classifier with validation data\n",
      "Prediction accuracy (%):  82.66666666666667\n",
      "confusion matrix is:\n",
      "[[ 89   5   2  14]\n",
      " [ 16 186  22   8]\n",
      " [ 10  20 147  27]\n",
      " [  1   4   1 198]]\n",
      "\n",
      "Running the classifier with test data\n",
      "Prediction accuracy (%):  82.0\n",
      "confusion matrix is:\n",
      "[[30  1  0  6]\n",
      " [ 2 34  5  2]\n",
      " [ 4  7 49  8]\n",
      " [ 0  1  0 51]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def KNNClassifier(X_train, y_train, X_test,k):\n",
    "\n",
    "    #Create KNN object\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k,weights='distance',p=2)\n",
    "    neigh.fit(X_train, y_train) # Fit KNN model\n",
    "\n",
    "    y_pred_knn = neigh.predict(X_test)\n",
    "    return y_pred_knn\n",
    "\n",
    "#print(X_train_prepared.shape) #debug line\n",
    "#print(X_test_prepared.shape) #debug line\n",
    "\n",
    "print('For k = 21')\n",
    "#Run the classifier with test data\n",
    "print('Running the classifier with validation data')\n",
    "y_pred_knn=KNNClassifier(X_train_prepared, y_train, X_validate_prepared,21)\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_knn == y_validate)/len(y_validate))\n",
    "cmx_knn = confusion_matrix(y_validate, y_pred_knn, labels=[\"no_tu\",\"glioma_t\",\"meningioma_tu\",\"pituitary_t\"])\n",
    "print('confusion matrix is:')\n",
    "print(cmx_knn)\n",
    "print('')\n",
    "\n",
    "#Run the classifier with test data\n",
    "print('Running the classifier with test data')\n",
    "y_pred_knn_test=KNNClassifier(X_train_prepared, y_train, X_test_prepared,21)\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_knn_test == y_test)/len(y_test))\n",
    "cmx_knn = confusion_matrix(y_test, y_pred_knn_test, labels=[\"no_tu\",\"glioma_t\",\"meningioma_tu\",\"pituitary_t\"])\n",
    "print('confusion matrix is:')\n",
    "print(cmx_knn)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dcfc3",
   "metadata": {},
   "source": [
    "Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd932bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum samples for split: 2\n",
      "Running the classifier with validation data\n",
      "Prediction accuracy (%):  71.86666666666666\n",
      "Tree depth:  14\n",
      "[[ 77  10  18   5]\n",
      " [ 17 163  41  11]\n",
      " [ 13  35 140  16]\n",
      " [ 13  16  16 159]]\n",
      "\n",
      "Running the classifier with test data\n",
      "Prediction accuracy (%):  68.0\n",
      "Tree depth:  14\n",
      "[[20  4  8  5]\n",
      " [ 2 31  9  1]\n",
      " [ 2 10 44 12]\n",
      " [ 3  2  6 41]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit the data\n",
    "tree_params={\n",
    "    'criterion': 'entropy',\n",
    "    'min_samples_split':2\n",
    "    }\n",
    "clf_tree = tree.DecisionTreeClassifier(**tree_params)\n",
    "clf_tree.fit(X_train_prepared, y_train)\n",
    "\n",
    "#print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf_tree.predict(X_train_prepared)))\n",
    "#print('Accuracy Score on test data: ', accuracy_score(y_true=y_test, y_pred=clf_tree.predict(X_validate_prepared)))\n",
    "\n",
    "print(\"Minimum samples for split: 2\")\n",
    "\n",
    "#Run the classifier with test data\n",
    "print('Running the classifier with validation data')\n",
    "y_pred_tree = clf_tree.predict(X_validate_prepared)\n",
    "#print(np.array(y_pred_tree == y_test)[:25])\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_tree == y_validate)/len(y_validate))\n",
    "print('Tree depth: ', clf_tree.get_depth())\n",
    "cmx_tree = confusion_matrix(y_validate, y_pred_tree, labels=[\"no_tu\",\"glioma_t\",\"meningioma_tu\",\"pituitary_t\"])\n",
    "print(cmx_tree)\n",
    "print('')\n",
    "\n",
    "#Run the classifier with test data\n",
    "print('Running the classifier with test data')\n",
    "y_pred_tree_test = clf_tree.predict(X_test_prepared)\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_tree_test == y_test)/len(y_test))\n",
    "print('Tree depth: ', clf_tree.get_depth())\n",
    "cmx_tree = confusion_matrix(y_test, y_pred_tree_test, labels=[\"no_tu\",\"glioma_t\",\"meningioma_tu\",\"pituitary_t\"])\n",
    "print(cmx_tree)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05ddd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
