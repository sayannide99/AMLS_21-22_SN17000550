{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac5008a",
   "metadata": {},
   "source": [
    "Final CODE FOR TASK A\n",
    "\n",
    "Import necessary libraries & functions for Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaba7afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import shutil\n",
    "\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e1c90",
   "metadata": {},
   "source": [
    "Tumour Image Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197105bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_formatting_needed():\n",
    "    #use this function if pickle file for sorted training dataset TaskA already exists\n",
    "    #Initialising Parameters for future steps\n",
    "    base_name = 'TaskA_training_dataset'\n",
    "    width = 256\n",
    "    include = {'no_tumour', 'tumour'}\n",
    "    \n",
    "def no_formatting_needed_testset():\n",
    "    #use this function if pickle file for sorted test dataset TaskA already exists\n",
    "    #Initialising Parameters for future steps\n",
    "    base_name = 'TaskA_test_dataset'\n",
    "    width = 256\n",
    "    include = {'no_tumour', 'tumour'}\n",
    "    print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f5d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_needed():\n",
    "    #use this function if pickle file for sorted trainig dataset TaskA does not exist (i.e first time of running the code)\n",
    "    \n",
    "    #Function to preprocess the tumour images (grayscale, resize, convert to array and storing in pickle file)\n",
    "    def format_img(src, pklname, include, width=150, height=None):\n",
    "        \"\"\"\n",
    "        function to load images from source, convert to grayscale, resize them and \n",
    "        write them as arrays to a dictionary with their respective labels. \n",
    "        The dictionary is written to a pickle file.\n",
    "     \n",
    "        Parameter\n",
    "        ---------\n",
    "        src: str\n",
    "            path to data\n",
    "        pklname: str\n",
    "            path to output file\n",
    "        width: int\n",
    "            target width of the image in pixels\n",
    "        include: set[str]\n",
    "            set containing str\n",
    "        \"\"\"\n",
    "     \n",
    "        height = height if height is not None else width\n",
    "     \n",
    "        data = dict()\n",
    "        data['label'] = []\n",
    "        data['filename'] = []\n",
    "        data['data'] = []   \n",
    "     \n",
    "        pklname = f\"{pklname}_{width}x{height}px.pkl\"\n",
    " \n",
    "        # read all images in PATH, convert to greyscale, resize and write to destination\n",
    "        for subdir in os.listdir(src):\n",
    "            if subdir in include:\n",
    "                print(subdir)\n",
    "                current_path = os.path.join(src, subdir)\n",
    " \n",
    "                for file in os.listdir(current_path):\n",
    "                    if file[-3:] in {'jpg', 'png'}:\n",
    "                        im = imread(os.path.join(current_path, file), as_gray=True) \n",
    "                        im = resize(im, (width, height)) \n",
    "                        data['label'].append(subdir[:-4])\n",
    "                        data['filename'].append(file)\n",
    "                        data['data'].append(im)\n",
    " \n",
    "            joblib.dump(data, pklname)\n",
    "    \n",
    "    #Sorting the images to two folders; no tumour and tumour\n",
    "    #Import label.csv file\n",
    "    data = pd.read_csv('./dataset./label.csv')\n",
    "    #print(data) #debug line\n",
    "\n",
    "    #Give the image labels binary values based on tumour (1) or no tumour (0)\n",
    "    data.label[data.label == 'no_tumor'] = 0\n",
    "    data.label[data.label == 'meningioma_tumor'] = 1\n",
    "    data.label[data.label == 'glioma_tumor'] = 1\n",
    "    data.label[data.label == 'pituitary_tumor'] = 1\n",
    "    #print(data.dtypes) #debug line\n",
    "\n",
    "    #Convert the label data type to integer\n",
    "    data['label'] = data['label'].astype(str).astype(int)\n",
    "    #print(data.dtypes) #debug line\n",
    "    #print(data) #debug line\n",
    "\n",
    "    #Create two arrays which contain the respective image names  \n",
    "    no_tumour = []\n",
    "    tumour = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        if data['label'][i] == 0:\n",
    "            no_tumour.append(data['file_name'][i])\n",
    "        else:\n",
    "            tumour.append(data['file_name'][i])\n",
    "\n",
    "    #print(len(tumour)) #debug line\n",
    "    #print(len(no_tumour)) #debug line\n",
    "\n",
    "    #Create a sorted subfolder to store the sorted images\n",
    "    os.mkdir(os.path.join(\"./dataset./image\", \"TaskA_sortedtrainingdataset\"))\n",
    "\n",
    "    #Create two subfolders in sorted folder\n",
    "    os.mkdir(os.path.join(\"./dataset./image/TaskA_sortedtrainingdataset\", \"no_tumour\"))\n",
    "    os.mkdir(os.path.join(\"./dataset./image/TaskA_sortedtrainingdataset\", \"tumour\"))\n",
    "\n",
    "    #Copy the images into the respective subfolders\n",
    "    src_dir = \"./dataset./image\"\n",
    "    dst_dir1 = os.path.join(\"./dataset./image/TaskA_sortedtrainingdataset\", \"no_tumour\")\n",
    "    dst_dir2 = os.path.join(\"./dataset./image/TaskA_sortedtrainingdataset\", \"tumour\")\n",
    "    data_path = \"./dataset./image/TaskA_sortedtrainingdataset\"\n",
    "\n",
    "    for imageName in no_tumour:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir1)\n",
    "\n",
    "    for imageName in tumour:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir2)\n",
    "\n",
    "    #os.listdir(data_path)  #debug line\n",
    "\n",
    "    #Initialising Parameters for resize function\n",
    "    base_name = 'TaskA_training_dataset'\n",
    "    width = 256\n",
    "    include = {'no_tumour', 'tumour'}\n",
    " \n",
    "    #Create pkl file with resized images resized and converted to arrays\n",
    "    format_img(src=data_path, pklname=base_name, width=width, include=include)\n",
    "    print(\"Step Complete: Training dataset formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cc100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_needed_testset():\n",
    "    #use this function if pickle file for sorted test dataset TaskA does not exist (i.e first time of running the code)\n",
    "    \n",
    "    #Function to preprocess the tumour images (grayscale, resize, convert to array and storing in pickle file)\n",
    "    def format_img_test(src, pklname, include, width=150, height=None):\n",
    "        \"\"\"\n",
    "        function to load images from source, convert to grayscale, resize them and \n",
    "        write them as arrays to a dictionary with their respective labels. \n",
    "        The dictionary is written to a pickle file.\n",
    "     \n",
    "        Parameter\n",
    "        ---------\n",
    "        src: str\n",
    "            path to data\n",
    "        pklname: str\n",
    "            path to output file\n",
    "        width: int\n",
    "            target width of the image in pixels\n",
    "        include: set[str]\n",
    "            set containing str\n",
    "        \"\"\"\n",
    "     \n",
    "        height = height if height is not None else width\n",
    "     \n",
    "        data = dict()\n",
    "        data['label'] = []\n",
    "        data['filename'] = []\n",
    "        data['data'] = []   \n",
    "     \n",
    "        pklname = f\"{pklname}_{width}x{height}px.pkl\"\n",
    " \n",
    "        # read all images in PATH, convert to greyscale, resize and write to destination\n",
    "        for subdir in os.listdir(src):\n",
    "            if subdir in include:\n",
    "                print(subdir)\n",
    "                current_path = os.path.join(src, subdir)\n",
    " \n",
    "                for file in os.listdir(current_path):\n",
    "                    if file[-3:] in {'jpg', 'png'}:\n",
    "                        im = imread(os.path.join(current_path, file), as_gray=True) \n",
    "                        im = resize(im, (width, height)) \n",
    "                        data['label'].append(subdir[:-4])\n",
    "                        data['filename'].append(file)\n",
    "                        data['data'].append(im)\n",
    " \n",
    "            joblib.dump(data, pklname)\n",
    "    \n",
    "    #Sorting the images to two folders; no tumour and tumour\n",
    "    #Import label.csv file\n",
    "    data = pd.read_csv('./test./label.csv')\n",
    "    #print(data) #debug line\n",
    "\n",
    "    #Give the image labels binary values based on tumour (1) or no tumour (0)\n",
    "    data.label[data.label == 'no_tumor'] = 0\n",
    "    data.label[data.label == 'meningioma_tumor'] = 1\n",
    "    data.label[data.label == 'glioma_tumor'] = 1\n",
    "    data.label[data.label == 'pituitary_tumor'] = 1\n",
    "    #print(data.dtypes) #debug line\n",
    "\n",
    "    #Convert the label data type to integer\n",
    "    data['label'] = data['label'].astype(str).astype(int)\n",
    "    #print(data.dtypes) #debug line\n",
    "    #print(data) #debug line\n",
    "\n",
    "    #Create two arrays which contain the respective image names  \n",
    "    no_tumour = []\n",
    "    tumour = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        if data['label'][i] == 0:\n",
    "            no_tumour.append(data['file_name'][i])\n",
    "        else:\n",
    "            tumour.append(data['file_name'][i])\n",
    "\n",
    "    #print(len(tumour)) #debug line\n",
    "    #print(len(no_tumour)) #debug line\n",
    "\n",
    "    #Create a sorted subfolder to store the sorted images\n",
    "    os.mkdir(os.path.join(\"./test./image\", \"TaskA_sortedtestdataset\"))\n",
    "\n",
    "    #Create two subfolders in sorted folder\n",
    "    os.mkdir(os.path.join(\"./test./image/TaskA_sortedtestdataset\", \"no_tumour\"))\n",
    "    os.mkdir(os.path.join(\"./test./image/TaskA_sortedtestdataset\", \"tumour\"))\n",
    "\n",
    "    #Copy the images into the respective subfolders\n",
    "    src_dir = \"./test./image\"\n",
    "    dst_dir1 = os.path.join(\"./test./image/TaskA_sortedtestdataset\", \"no_tumour\")\n",
    "    dst_dir2 = os.path.join(\"./test./image/TaskA_sortedtestdataset\", \"tumour\")\n",
    "    data_path = \"./test./image/TaskA_sortedtestdataset\"\n",
    "\n",
    "    for imageName in no_tumour:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir1)\n",
    "\n",
    "    for imageName in tumour:\n",
    "        shutil.copy(os.path.join(src_dir, imageName), dst_dir2)\n",
    "\n",
    "    #os.listdir(data_path)  #debug line\n",
    "\n",
    "    #Initialising Parameters for resize function\n",
    "    base_name = 'TaskA_test_dataset'\n",
    "    width = 256\n",
    "    include = {'no_tumour', 'tumour'}\n",
    " \n",
    "    #Create pkl file with resized images resized and converted to arrays\n",
    "    format_img_test(src=data_path, pklname=base_name, width=width, include=include)\n",
    "    print(\"Step Complete: Test dataset formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfad0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does a pickle file containing the sorted training data exist?(Please enter N in captial if running code for first time and Y in capital for subsequent runs) N\n",
      "no_tumour\n",
      "tumour\n",
      "Step Complete: Training dataset formatted\n",
      "no_tumour\n",
      "tumour\n",
      "Step Complete: Test dataset formatted\n"
     ]
    }
   ],
   "source": [
    "image_formatting=input(\"Does a pickle file containing the sorted training data exist?(Please enter N in captial if running code for first time and Y in capital for subsequent runs) \")\n",
    "\n",
    "if image_formatting == \"Y\":\n",
    "    no_formatting_needed()\n",
    "    no_formatting_needed_testset()\n",
    "elif image_formatting == \"N\":\n",
    "    formatting_needed()\n",
    "    formatting_needed_testset()\n",
    "else:\n",
    "    print(\"Incorrect input! Please run cell again and input either Y or N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9fd3a",
   "metadata": {},
   "source": [
    "Load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b0543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset\n",
      "number of samples:  3000\n",
      "image resolution:  (256, 256)\n",
      "labels: ['no_tu' 'tu']\n",
      "\n",
      "Test dataset\n",
      "number of samples:  200\n",
      "image resolution:  (256, 256)\n",
      "labels: ['no_tu' 'tu']\n",
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "#Initialising some parameters\n",
    "base_name = 'TaskA_training_dataset'\n",
    "width = 256\n",
    "include = {'no_tumour', 'tumour'}\n",
    "\n",
    "#Load pickle file\n",
    "data = joblib.load(f'{base_name}_{width}x{width}px.pkl')\n",
    "\n",
    "#some information about the data\n",
    "print('Training dataset')\n",
    "print('number of samples: ', len(data['data']))\n",
    "print('image resolution: ', data['data'][0].shape)\n",
    "print('labels:', np.unique(data['label']))\n",
    "\n",
    "#Initialising some parameters\n",
    "base_name = 'TaskA_test_dataset'\n",
    "width = 256\n",
    "include = {'no_tumour', 'tumour'}\n",
    "\n",
    "#Load pickle file\n",
    "data_test = joblib.load(f'{base_name}_{width}x{width}px.pkl')\n",
    "\n",
    "#some information about the data\n",
    "print('')\n",
    "print('Test dataset')\n",
    "print('number of samples: ', len(data_test['data']))\n",
    "print('image resolution: ', data_test['data'][0].shape)\n",
    "print('labels:', np.unique(data_test['label']))\n",
    "\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17025b5f",
   "metadata": {},
   "source": [
    "Split the data to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e963052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "#Extract the X and y values from training data\n",
    "X = np.array(data['data'])\n",
    "y = np.array(data['label'])\n",
    "\n",
    "#Split data to train and test (75% train, 25% split)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.25, \n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "#Extract the X and y values from the test data\n",
    "X_test = np.array(data_test['data'])\n",
    "y_test = np.array(data_test['label'])\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a58c4",
   "metadata": {},
   "source": [
    "Define the HOG transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eb49e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "class HogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Expects an array of 2d arrays (greyscale or 1 channel images)\n",
    "    Calculates hog features for each image\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, y=None, orientations=9,\n",
    "                 pixels_per_cell=(8, 8),\n",
    "                 cells_per_block=(3, 3), block_norm='L2-Hys'):\n",
    "        self.y = y\n",
    "        self.orientations = orientations\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "        self.block_norm = block_norm\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    " \n",
    "        def local_hog(X):\n",
    "            return hog(X,\n",
    "                       orientations=self.orientations,\n",
    "                       pixels_per_cell=self.pixels_per_cell,\n",
    "                       cells_per_block=self.cells_per_block,\n",
    "                       block_norm=self.block_norm)\n",
    " \n",
    "        try: # parallel\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "        except:\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55110f44",
   "metadata": {},
   "source": [
    "Prepare training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554530cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Complete\n"
     ]
    }
   ],
   "source": [
    "# create an instance of HOG transformer and scaling transformer\n",
    "hogify = HogTransformer(\n",
    "    #change parameters below for optimisation\n",
    "    pixels_per_cell=(10, 10), \n",
    "    cells_per_block=(2, 2), \n",
    "    orientations=9, \n",
    "    block_norm='L2-Hys'\n",
    ")\n",
    "scalify = StandardScaler()\n",
    " \n",
    "# call fit_transform on each transform converting X_train step by step\n",
    "X_train_hog = hogify.fit_transform(X_train)\n",
    "X_train_prepared = scalify.fit_transform(X_train_hog)\n",
    "#print(X_train.shape) #debug line\n",
    "#print(X_train_prepared.shape) #debug line\n",
    "\n",
    "#transform validation data\n",
    "X_validate_hog = hogify.transform(X_validate)\n",
    "X_validate_prepared = scalify.transform(X_validate_hog)\n",
    "#print(X_validate_prepared.shape) #debug line\n",
    "\n",
    "#transform test data\n",
    "X_test_hog = hogify.transform(X_test)\n",
    "X_test_prepared = scalify.transform(X_test_hog)\n",
    "#print(X_test_prepared.shape) #debug line\n",
    "print(\"Step Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f939f",
   "metadata": {},
   "source": [
    "SVM with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9790677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1949.82, NNZs: 20736, Bias: 1449.088980, T: 2250, Avg. loss: 3477.441878\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1196.84, NNZs: 20736, Bias: 1645.958153, T: 4500, Avg. loss: 430.582401\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 847.91, NNZs: 20736, Bias: 1685.127947, T: 6750, Avg. loss: 60.666984\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 656.22, NNZs: 20736, Bias: 1693.437445, T: 9000, Avg. loss: 9.975578\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 530.13, NNZs: 20736, Bias: 1694.041626, T: 11250, Avg. loss: 1.095790\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 446.69, NNZs: 20736, Bias: 1693.736454, T: 13500, Avg. loss: 0.429697\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 388.27, NNZs: 20736, Bias: 1693.316661, T: 15750, Avg. loss: 0.489682\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 341.38, NNZs: 20736, Bias: 1693.196592, T: 18000, Avg. loss: 0.021031\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 312.32, NNZs: 20736, Bias: 1692.260080, T: 20250, Avg. loss: 0.824692\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 284.67, NNZs: 20736, Bias: 1691.901754, T: 22500, Avg. loss: 0.296004\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 260.87, NNZs: 20736, Bias: 1691.815652, T: 24750, Avg. loss: 0.090216\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 245.07, NNZs: 20736, Bias: 1691.278868, T: 27000, Avg. loss: 0.395000\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 228.33, NNZs: 20736, Bias: 1691.067839, T: 29250, Avg. loss: 0.263424\n",
      "Total training time: 0.81 seconds.\n",
      "Convergence after 13 epochs took 0.81 seconds\n",
      "\n",
      "Running the classifier with validation data\n",
      "Prediction accuracy (%):  96.66666666666667\n",
      "[[108  19]\n",
      " [  6 617]]\n",
      "\n",
      "Running the classifier with test data\n",
      "Prediction accuracy (%):  97.0\n",
      "[[ 31   6]\n",
      " [  0 163]]\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-3, verbose=1, learning_rate='optimal', alpha=0.0005)\n",
    "\n",
    "#Fit the data\n",
    "sgd_clf.fit(X_train_prepared, y_train)\n",
    "\n",
    "#Validate the data\n",
    "y_pred = sgd_clf.predict(X_validate_prepared)\n",
    "#print(np.array(y_pred == y_validate)[:25])\n",
    "print('')\n",
    "print('Running the classifier with validation data')\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred == y_validate)/len(y_validate))\n",
    "cmx = confusion_matrix(y_validate, y_pred, labels=[\"no_tu\",\"tu\"])\n",
    "print(cmx)\n",
    "\n",
    "#Run the classifier with test data\n",
    "y_pred_test = sgd_clf.predict(X_test_prepared)\n",
    "#print(np.array(y_pred_test == y_test)[:25])\n",
    "print('')\n",
    "print('Running the classifier with test data')\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_test == y_test)/len(y_test))\n",
    "cmx = confusion_matrix(y_test, y_pred_test, labels=[\"no_tu\",\"tu\"])\n",
    "print(cmx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be926eb9",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea3e2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 24\n",
      "Running the classifier with validation data\n",
      "Prediction accuracy (%):  94.4\n",
      "confusion matrix is:\n",
      "[[101  26]\n",
      " [ 16 607]]\n",
      "\n",
      "Running the classifier with test data\n",
      "Prediction accuracy (%):  93.0\n",
      "confusion matrix is:\n",
      "[[ 28   9]\n",
      " [  5 158]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def KNNClassifier(X_train, y_train, X_test,k):\n",
    "    #Create KNN object\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k,weights='distance',p=2)\n",
    "    neigh.fit(X_train, y_train) # Fit KNN model\n",
    "\n",
    "    y_pred_knn = neigh.predict(X_test)\n",
    "    return y_pred_knn\n",
    "\n",
    "#print(X_train_prepared.shape) #debug line\n",
    "#print(X_validate_prepared.shape) #debug line\n",
    "\n",
    "print('For k = 24')\n",
    "#Run the classifier with test data\n",
    "print('Running the classifier with validation data')\n",
    "y_pred_knn=KNNClassifier(X_train_prepared, y_train, X_validate_prepared,24)\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_knn == y_validate)/len(y_validate))\n",
    "cmx_knn = confusion_matrix(y_validate, y_pred_knn, labels=[\"no_tu\",\"tu\"])\n",
    "print('confusion matrix is:')\n",
    "print(cmx_knn)\n",
    "print('')\n",
    "\n",
    "#Run the classifier with test data\n",
    "print('Running the classifier with test data')\n",
    "y_pred_knn_test=KNNClassifier(X_train_prepared, y_train, X_test_prepared,24)\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_knn_test == y_test)/len(y_test))\n",
    "cmx_knn = confusion_matrix(y_test, y_pred_knn_test, labels=[\"no_tu\",\"tu\"])\n",
    "print('confusion matrix is:')\n",
    "print(cmx_knn)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc6956",
   "metadata": {},
   "source": [
    "Decision Trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bccf159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the classifier with validation data\n",
      "Minimum samples for split: 2\n",
      "Prediction accuracy (%):  91.2\n",
      "Tree depth:  10\n",
      "[[ 94  33]\n",
      " [ 33 590]]\n",
      "\n",
      "Running the classifier with test data\n",
      "Minimum samples for split: 2\n",
      "Prediction accuracy (%):  95.5\n",
      "Tree depth:  10\n",
      "[[ 34   3]\n",
      " [  6 157]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit the data\n",
    "tree_params={\n",
    "    'criterion': 'entropy',\n",
    "    'min_samples_split':2\n",
    "    }\n",
    "clf_tree = tree.DecisionTreeClassifier(**tree_params)\n",
    "clf_tree.fit(X_train_prepared, y_train)\n",
    "\n",
    "#print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf_tree.predict(X_train_prepared)))\n",
    "#print('Accuracy Score on test data: ', accuracy_score(y_true=y_validate, y_pred=clf_tree.predict(X_validate_prepared)))\n",
    "\n",
    "print(\"Minimum samples for split: 2\")\n",
    "\n",
    "#Run the classifier with test data\n",
    "print('Running the classifier with validation data')\n",
    "y_pred_tree = clf_tree.predict(X_validate_prepared)\n",
    "#print(np.array(y_pred_tree == y_validate)[:25])\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_tree == y_validate)/len(y_validate))\n",
    "print('Tree depth: ', clf_tree.get_depth())\n",
    "cmx_tree = confusion_matrix(y_validate, y_pred_tree, labels=[\"no_tu\",\"tu\"])\n",
    "print(cmx_tree)\n",
    "print('')\n",
    "\n",
    "#Run the classifier with test data\n",
    "print('Running the classifier with test data')\n",
    "y_pred_tree_test = clf_tree.predict(X_test_prepared)\n",
    "print('Prediction accuracy (%): ', 100*np.sum(y_pred_tree_test == y_test)/len(y_test))\n",
    "print('Tree depth: ', clf_tree.get_depth())\n",
    "cmx_tree = confusion_matrix(y_test, y_pred_tree_test, labels=[\"no_tu\",\"tu\"])\n",
    "print(cmx_tree)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab3dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
